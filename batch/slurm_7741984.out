player initial finish
=> Restore ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-10000.pth
episode: 10500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-10500.pth
Episode: 10500 Reward: 217.929 Loss: 8.742
episode: 11000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-11000.pth
Episode: 11000 Reward: 298.034 Loss: 7.627
episode: 11500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-11500.pth
Episode: 11500 Reward: 252.614 Loss: 9.956
episode: 12000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-12000.pth
Episode: 12000 Reward: 260.602 Loss: 9.422
episode: 12500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-12500.pth
Episode: 12500 Reward: 159.865 Loss: 9.523
episode: 13000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-13000.pth
Episode: 13000 Reward: 323.963 Loss: 11.588
episode: 13500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-13500.pth
Episode: 13500 Reward: 273.614 Loss: 9.250
episode: 14000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-14000.pth
Episode: 14000 Reward: 330.269 Loss: 10.793
episode: 14500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-14500.pth
Episode: 14500 Reward: 307.885 Loss: 8.592
episode: 15000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-15000.pth
Episode: 15000 Reward: 273.321 Loss: 7.042
episode: 15500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-15500.pth
Episode: 15500 Reward: 70.743 Loss: 10.220
episode: 16000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-16000.pth
Episode: 16000 Reward: 280.897 Loss: 4.328
episode: 16500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-16500.pth
Episode: 16500 Reward: 347.488 Loss: 3.819
episode: 17000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-17000.pth
Episode: 17000 Reward: 234.717 Loss: 7.009
episode: 17500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-17500.pth
Episode: 17500 Reward: 86.647 Loss: 4.775
episode: 18000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-18000.pth
Episode: 18000 Reward: 377.285 Loss: 10.357
episode: 18500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-18500.pth
Episode: 18500 Reward: 427.182 Loss: 4.797
episode: 19000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-19000.pth
Episode: 19000 Reward: 388.124 Loss: 13.026
episode: 19500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-19500.pth
Episode: 19500 Reward: 324.211 Loss: 9.287
episode: 20000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-20000.pth
Episode: 20000 Reward: 424.225 Loss: 14.466
episode: 20500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-20500.pth
Episode: 20500 Reward: 409.427 Loss: 6.295
episode: 21000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-21000.pth
Episode: 21000 Reward: 380.480 Loss: 20.859
episode: 21500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-21500.pth
Episode: 21500 Reward: 414.044 Loss: 9.113
episode: 22000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-22000.pth
Episode: 22000 Reward: 377.841 Loss: 6.769
episode: 22500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-22500.pth
Episode: 22500 Reward: 375.806 Loss: 7.234
episode: 23000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-23000.pth
Episode: 23000 Reward: 375.753 Loss: 6.994
episode: 23500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-23500.pth
Episode: 23500 Reward: 273.296 Loss: 6.703
episode: 24000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-24000.pth
Episode: 24000 Reward: 357.455 Loss: 4.806
episode: 24500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-24500.pth
Episode: 24500 Reward: 335.593 Loss: 6.225
episode: 25000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-25000.pth
Episode: 25000 Reward: 132.364 Loss: 5.888
episode: 25500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-25500.pth
Episode: 25500 Reward: 358.598 Loss: 2.895
episode: 26000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-26000.pth
Episode: 26000 Reward: 128.344 Loss: 7.620
episode: 26500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-26500.pth
Episode: 26500 Reward: 391.165 Loss: 8.678
episode: 27000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-27000.pth
Episode: 27000 Reward: 397.022 Loss: 7.357
episode: 27500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-27500.pth
Episode: 27500 Reward: 395.127 Loss: 4.720
episode: 28000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-28000.pth
Episode: 28000 Reward: 123.603 Loss: 8.284
episode: 28500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-28500.pth
Episode: 28500 Reward: 272.879 Loss: 4.646
episode: 29000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-29000.pth
Episode: 29000 Reward: 221.685 Loss: 4.129
episode: 29500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-29500.pth
Episode: 29500 Reward: 137.635 Loss: 4.207
episode: 30000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-30000.pth
Episode: 30000 Reward: 371.576 Loss: 3.942
episode: 30500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-30500.pth
Episode: 30500 Reward: 348.607 Loss: 5.528
episode: 31000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-31000.pth
Episode: 31000 Reward: 360.658 Loss: 4.041
episode: 31500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-31500.pth
Episode: 31500 Reward: 387.685 Loss: 6.839
episode: 32000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-32000.pth
Episode: 32000 Reward: 323.142 Loss: 6.229
episode: 32500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-32500.pth
Episode: 32500 Reward: 266.921 Loss: 6.241
episode: 33000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-33000.pth
Episode: 33000 Reward: 336.450 Loss: 5.180
episode: 33500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-33500.pth
Episode: 33500 Reward: 313.477 Loss: 9.171
episode: 34000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-34000.pth
Episode: 34000 Reward: 394.549 Loss: 5.526
episode: 34500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-34500.pth
Episode: 34500 Reward: 397.588 Loss: 3.998
episode: 35000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-35000.pth
Episode: 35000 Reward: 351.113 Loss: 8.487
episode: 35500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-35500.pth
Episode: 35500 Reward: 203.991 Loss: 8.829
episode: 36000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-36000.pth
Episode: 36000 Reward: 311.062 Loss: 7.358
episode: 36500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-36500.pth
Episode: 36500 Reward: 387.127 Loss: 3.254
episode: 37000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-37000.pth
Episode: 37000 Reward: 387.168 Loss: 5.277
episode: 37500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-37500.pth
Episode: 37500 Reward: 93.604 Loss: 4.514
episode: 38000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-38000.pth
Episode: 38000 Reward: 353.749 Loss: 2.985
episode: 38500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-38500.pth
Episode: 38500 Reward: 338.275 Loss: 9.146
episode: 39000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-39000.pth
Episode: 39000 Reward: 375.712 Loss: 6.314
episode: 39500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-39500.pth
Episode: 39500 Reward: 247.471 Loss: 3.210
episode: 40000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-40000.pth
Episode: 40000 Reward: 321.132 Loss: 5.727
episode: 40500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-40500.pth
Episode: 40500 Reward: 361.640 Loss: 4.737
episode: 41000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-41000.pth
Episode: 41000 Reward: 206.208 Loss: 4.947
episode: 41500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-41500.pth
Episode: 41500 Reward: 279.842 Loss: 5.878
episode: 42000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-42000.pth
Episode: 42000 Reward: 148.000 Loss: 5.675
episode: 42500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-42500.pth
Episode: 42500 Reward: 445.333 Loss: 8.284
episode: 43000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-43000.pth
Episode: 43000 Reward: 356.574 Loss: 8.987
episode: 43500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-43500.pth
Episode: 43500 Reward: 343.335 Loss: 9.581
episode: 44000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-44000.pth
Episode: 44000 Reward: 120.882 Loss: 7.402
episode: 44500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-44500.pth
Episode: 44500 Reward: 441.256 Loss: 3.438
episode: 45000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-45000.pth
Episode: 45000 Reward: 443.640 Loss: 7.862
episode: 45500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-45500.pth
Episode: 45500 Reward: 129.674 Loss: 11.483
episode: 46000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-46000.pth
Episode: 46000 Reward: 445.234 Loss: 7.156
episode: 46500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-46500.pth
Episode: 46500 Reward: 401.363 Loss: 11.670
episode: 47000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-47000.pth
Episode: 47000 Reward: 166.466 Loss: 11.486
episode: 47500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-47500.pth
Episode: 47500 Reward: 352.014 Loss: 19.366
episode: 48000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-48000.pth
Episode: 48000 Reward: 383.242 Loss: 12.306
episode: 48500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-48500.pth
Episode: 48500 Reward: 363.676 Loss: 12.224
episode: 49000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-49000.pth
Episode: 49000 Reward: 377.952 Loss: 7.827
episode: 49500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-49500.pth
Episode: 49500 Reward: 430.455 Loss: 9.242
episode: 50000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-50000.pth
Episode: 50000 Reward: 434.322 Loss: 10.984
