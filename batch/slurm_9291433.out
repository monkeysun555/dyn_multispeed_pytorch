player initial finish
Episode starts from:  1
episode: 500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-500.pth
Episode: 500 Reward: -1600.311 Loss: 42.115
episode: 1000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-1000.pth
Episode: 1000 Reward: -3356.048 Loss: 62.195
episode: 1500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-1500.pth
Episode: 1500 Reward: -1241.229 Loss: 52.326
episode: 2000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-2000.pth
Episode: 2000 Reward: -995.087 Loss: 77.872
episode: 2500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-2500.pth
Episode: 2500 Reward: -529.939 Loss: 97.646
episode: 3000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-3000.pth
Episode: 3000 Reward: -634.082 Loss: 339.229
episode: 3500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-3500.pth
Episode: 3500 Reward: -3411.143 Loss: 421.376
episode: 4000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-4000.pth
Episode: 4000 Reward: -1009.120 Loss: 48.127
episode: 4500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-4500.pth
Episode: 4500 Reward: -498.846 Loss: 35.561
episode: 5000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-5000.pth
Episode: 5000 Reward: -1001.539 Loss: 132.428
episode: 5500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-5500.pth
Episode: 5500 Reward: -997.050 Loss: 33.808
episode: 6000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-6000.pth
Episode: 6000 Reward: -392.634 Loss: 87.406
episode: 6500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-6500.pth
Episode: 6500 Reward: -434.449 Loss: 44.713
episode: 7000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-7000.pth
Episode: 7000 Reward: -480.169 Loss: 63.661
episode: 7500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-7500.pth
Episode: 7500 Reward: -737.453 Loss: 42.150
episode: 8000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-8000.pth
Episode: 8000 Reward: -2921.524 Loss: 236.432
episode: 8500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-8500.pth
Episode: 8500 Reward: -545.581 Loss: 124.759
episode: 9000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-9000.pth
Episode: 9000 Reward: -2845.132 Loss: 199.881
episode: 9500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-9500.pth
Episode: 9500 Reward: -570.700 Loss: 143.422
episode: 10000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-10000.pth
Episode: 10000 Reward: -574.496 Loss: 40.904
episode: 10500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-10500.pth
Episode: 10500 Reward: -2634.340 Loss: 103.032
episode: 11000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-11000.pth
Episode: 11000 Reward: -1967.144 Loss: 222.970
episode: 11500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-11500.pth
Episode: 11500 Reward: -480.016 Loss: 35.538
episode: 12000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-12000.pth
Episode: 12000 Reward: -977.513 Loss: 55.804
episode: 12500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-12500.pth
Episode: 12500 Reward: -368.038 Loss: 67.790
episode: 13000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-13000.pth
Episode: 13000 Reward: -378.167 Loss: 34.955
episode: 13500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-13500.pth
Episode: 13500 Reward: -477.853 Loss: 61.597
episode: 14000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-14000.pth
Episode: 14000 Reward: -299.581 Loss: 136.345
episode: 14500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-14500.pth
Episode: 14500 Reward: -1930.642 Loss: 58.859
episode: 15000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-15000.pth
Episode: 15000 Reward: -1461.630 Loss: 42.706
episode: 15500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-15500.pth
Episode: 15500 Reward: -244.938 Loss: 67.087
episode: 16000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-16000.pth
Episode: 16000 Reward: -644.015 Loss: 32.313
episode: 16500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-16500.pth
Episode: 16500 Reward: -2349.305 Loss: 99.543
episode: 17000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-17000.pth
Episode: 17000 Reward: -95.096 Loss: 63.548
episode: 17500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-17500.pth
Episode: 17500 Reward: -398.745 Loss: 71.642
episode: 18000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-18000.pth
Episode: 18000 Reward: -86.014 Loss: 84.003
episode: 18500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-18500.pth
Episode: 18500 Reward: -217.770 Loss: 60.679
episode: 19000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-19000.pth
Episode: 19000 Reward: -298.007 Loss: 51.329
episode: 19500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-19500.pth
Episode: 19500 Reward: -407.439 Loss: 70.177
episode: 20000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-20000.pth
Episode: 20000 Reward: -258.292 Loss: 63.357
episode: 20500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-20500.pth
Episode: 20500 Reward: -284.639 Loss: 53.248
episode: 21000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-21000.pth
Episode: 21000 Reward: -29.004 Loss: 73.979
episode: 21500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-21500.pth
Episode: 21500 Reward: -102.950 Loss: 32.224
episode: 22000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-22000.pth
Episode: 22000 Reward: -47.292 Loss: 72.319
episode: 22500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-22500.pth
Episode: 22500 Reward: -436.912 Loss: 39.909
episode: 23000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-23000.pth
Episode: 23000 Reward: -368.699 Loss: 72.169
episode: 23500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-23500.pth
Episode: 23500 Reward: -8.233 Loss: 27.501
episode: 24000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-24000.pth
Episode: 24000 Reward: -1411.147 Loss: 64.822
episode: 24500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-24500.pth
Episode: 24500 Reward: -20.408 Loss: 63.941
episode: 25000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-25000.pth
Episode: 25000 Reward: -114.838 Loss: 52.262
episode: 25500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-25500.pth
Episode: 25500 Reward: -434.101 Loss: 44.895
episode: 26000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-26000.pth
Episode: 26000 Reward: -2096.706 Loss: 105.015
episode: 26500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-26500.pth
Episode: 26500 Reward: 44.050 Loss: 37.615
episode: 27000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-27000.pth
Episode: 27000 Reward: -1941.805 Loss: 70.309
episode: 27500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-27500.pth
Episode: 27500 Reward: -1697.849 Loss: 37.426
episode: 28000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-28000.pth
Episode: 28000 Reward: 91.408 Loss: 19.189
episode: 28500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-28500.pth
Episode: 28500 Reward: -1644.774 Loss: 77.229
episode: 29000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-29000.pth
Episode: 29000 Reward: -1080.114 Loss: 46.832
episode: 29500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-29500.pth
Episode: 29500 Reward: -159.389 Loss: 63.646
episode: 30000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-30000.pth
Episode: 30000 Reward: 105.168 Loss: 23.776
episode: 30500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-30500.pth
Episode: 30500 Reward: -875.418 Loss: 23.301
episode: 31000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-31000.pth
Episode: 31000 Reward: -19.930 Loss: 76.946
episode: 31500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-31500.pth
Episode: 31500 Reward: -183.648 Loss: 56.640
episode: 32000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-32000.pth
Episode: 32000 Reward: -260.892 Loss: 29.230
episode: 32500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-32500.pth
Episode: 32500 Reward: 64.562 Loss: 61.249
episode: 33000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-33000.pth
Episode: 33000 Reward: -31.821 Loss: 54.757
episode: 33500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-33500.pth
Episode: 33500 Reward: 149.577 Loss: 44.009
episode: 34000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-34000.pth
Episode: 34000 Reward: 198.445 Loss: 52.071
episode: 34500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-34500.pth
Episode: 34500 Reward: 177.276 Loss: 33.658
episode: 35000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-35000.pth
Episode: 35000 Reward: -7.675 Loss: 74.592
episode: 35500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-35500.pth
Episode: 35500 Reward: 21.268 Loss: 86.197
episode: 36000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-36000.pth
Episode: 36000 Reward: 183.504 Loss: 34.569
episode: 36500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-36500.pth
Episode: 36500 Reward: -376.431 Loss: 59.370
episode: 37000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-37000.pth
Episode: 37000 Reward: -647.251 Loss: 92.626
episode: 37500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-37500.pth
Episode: 37500 Reward: -129.962 Loss: 65.723
episode: 38000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-38000.pth
Episode: 38000 Reward: 138.490 Loss: 52.237
episode: 38500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-38500.pth
Episode: 38500 Reward: 279.541 Loss: 98.292
episode: 39000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-39000.pth
Episode: 39000 Reward: 279.162 Loss: 92.682
episode: 39500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-39500.pth
Episode: 39500 Reward: 109.824 Loss: 67.528
episode: 40000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-40000.pth
Episode: 40000 Reward: -80.989 Loss: 74.222
episode: 40500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-40500.pth
Episode: 40500 Reward: -626.241 Loss: 108.171
episode: 41000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-41000.pth
Episode: 41000 Reward: 153.961 Loss: 114.603
episode: 41500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-41500.pth
Episode: 41500 Reward: -517.704 Loss: 126.147
episode: 42000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-42000.pth
Episode: 42000 Reward: 213.452 Loss: 132.664
episode: 42500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-42500.pth
Episode: 42500 Reward: -222.495 Loss: 145.047
episode: 43000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-43000.pth
Episode: 43000 Reward: 352.408 Loss: 102.290
episode: 43500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-43500.pth
Episode: 43500 Reward: 273.947 Loss: 168.788
episode: 44000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-44000.pth
Episode: 44000 Reward: 275.769 Loss: 73.463
episode: 44500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-44500.pth
Episode: 44500 Reward: -73.441 Loss: 29.382
episode: 45000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-45000.pth
Episode: 45000 Reward: 272.161 Loss: 78.585
episode: 45500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-45500.pth
Episode: 45500 Reward: 227.440 Loss: 26.833
episode: 46000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-46000.pth
Episode: 46000 Reward: 285.099 Loss: 118.660
episode: 46500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-46500.pth
Episode: 46500 Reward: -46.006 Loss: 81.313
episode: 47000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-47000.pth
Episode: 47000 Reward: -851.853 Loss: 85.799
episode: 47500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-47500.pth
Episode: 47500 Reward: -245.550 Loss: 65.648
episode: 48000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-48000.pth
Episode: 48000 Reward: 355.600 Loss: 19.096
episode: 48500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-48500.pth
Episode: 48500 Reward: 471.609 Loss: 53.497
episode: 49000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-49000.pth
Episode: 49000 Reward: 341.601 Loss: 24.356
episode: 49500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-49500.pth
Episode: 49500 Reward: 195.431 Loss: 27.009
episode: 50000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-50000.pth
Episode: 50000 Reward: 433.466 Loss: 94.623
episode: 50500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-50500.pth
Episode: 50500 Reward: -164.660 Loss: 38.635
episode: 51000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-51000.pth
Episode: 51000 Reward: 358.746 Loss: 93.091
episode: 51500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-51500.pth
Episode: 51500 Reward: 322.166 Loss: 31.587
episode: 52000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-52000.pth
Episode: 52000 Reward: -650.955 Loss: 67.037
episode: 52500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-52500.pth
Episode: 52500 Reward: 300.847 Loss: 24.932
episode: 53000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-53000.pth
Episode: 53000 Reward: 351.071 Loss: 25.900
episode: 53500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-53500.pth
Episode: 53500 Reward: 230.342 Loss: 93.874
episode: 54000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-54000.pth
Episode: 54000 Reward: -32.377 Loss: 37.095
episode: 54500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-54500.pth
Episode: 54500 Reward: -709.461 Loss: 76.638
episode: 55000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-55000.pth
Episode: 55000 Reward: -441.571 Loss: 153.645
episode: 55500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-55500.pth
Episode: 55500 Reward: 205.541 Loss: 38.998
episode: 56000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-56000.pth
Episode: 56000 Reward: -414.374 Loss: 200.837
episode: 56500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-56500.pth
Episode: 56500 Reward: -252.030 Loss: 24.408
episode: 57000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-57000.pth
Episode: 57000 Reward: 5.098 Loss: 104.729
episode: 57500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-57500.pth
Episode: 57500 Reward: 513.640 Loss: 130.316
episode: 58000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-58000.pth
Episode: 58000 Reward: 334.850 Loss: 19.547
episode: 58500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-58500.pth
Episode: 58500 Reward: 423.836 Loss: 20.121
episode: 59000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-59000.pth
Episode: 59000 Reward: 393.824 Loss: 14.530
episode: 59500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-59500.pth
Episode: 59500 Reward: 592.574 Loss: 142.354
episode: 60000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-60000.pth
Episode: 60000 Reward: -578.812 Loss: 34.035
episode: 60500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-60500.pth
Episode: 60500 Reward: -448.170 Loss: 80.180
episode: 61000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-61000.pth
Episode: 61000 Reward: 399.276 Loss: 27.442
episode: 61500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-61500.pth
Episode: 61500 Reward: 461.461 Loss: 14.455
episode: 62000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-62000.pth
Episode: 62000 Reward: 462.523 Loss: 19.197
episode: 62500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-62500.pth
Episode: 62500 Reward: 419.111 Loss: 23.362
episode: 63000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-63000.pth
Episode: 63000 Reward: 330.701 Loss: 92.230
episode: 63500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-63500.pth
Episode: 63500 Reward: -177.732 Loss: 42.305
episode: 64000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-64000.pth
Episode: 64000 Reward: 528.072 Loss: 19.947
episode: 64500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-64500.pth
Episode: 64500 Reward: 369.188 Loss: 32.483
episode: 65000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-65000.pth
Episode: 65000 Reward: 126.441 Loss: 70.395
episode: 65500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-65500.pth
Episode: 65500 Reward: 556.622 Loss: 17.223
episode: 66000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-66000.pth
Episode: 66000 Reward: 398.851 Loss: 19.838
episode: 66500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-66500.pth
Episode: 66500 Reward: -288.379 Loss: 43.725
episode: 67000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-67000.pth
Episode: 67000 Reward: 457.352 Loss: 75.968
episode: 67500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-67500.pth
Episode: 67500 Reward: 122.547 Loss: 104.585
episode: 68000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-68000.pth
Episode: 68000 Reward: 569.614 Loss: 66.228
episode: 68500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-68500.pth
Episode: 68500 Reward: -142.596 Loss: 159.165
episode: 69000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-69000.pth
Episode: 69000 Reward: 489.735 Loss: 18.736
episode: 69500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-69500.pth
Episode: 69500 Reward: -52.021 Loss: 38.694
episode: 70000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-70000.pth
Episode: 70000 Reward: 188.557 Loss: 74.928
episode: 70500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-70500.pth
Episode: 70500 Reward: 283.001 Loss: 91.782
episode: 71000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-71000.pth
Episode: 71000 Reward: 493.793 Loss: 30.731
episode: 71500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-71500.pth
Episode: 71500 Reward: 435.998 Loss: 141.225
episode: 72000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-72000.pth
Episode: 72000 Reward: -50.054 Loss: 13.975
episode: 72500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-72500.pth
Episode: 72500 Reward: 499.767 Loss: 19.212
episode: 73000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-73000.pth
Episode: 73000 Reward: -50.251 Loss: 13.641
episode: 73500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-73500.pth
Episode: 73500 Reward: 466.240 Loss: 19.834
episode: 74000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-74000.pth
Episode: 74000 Reward: 147.702 Loss: 20.537
episode: 74500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-74500.pth
Episode: 74500 Reward: 523.148 Loss: 15.919
episode: 75000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-75000.pth
Episode: 75000 Reward: 552.579 Loss: 54.678
episode: 75500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-75500.pth
Episode: 75500 Reward: 420.440 Loss: 136.778
episode: 76000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-76000.pth
Episode: 76000 Reward: -176.743 Loss: 16.382
episode: 76500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-76500.pth
Episode: 76500 Reward: 2.713 Loss: 132.840
episode: 77000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-77000.pth
Episode: 77000 Reward: 240.734 Loss: 20.230
episode: 77500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-77500.pth
Episode: 77500 Reward: 404.751 Loss: 75.402
episode: 78000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-78000.pth
Episode: 78000 Reward: -525.834 Loss: 170.256
episode: 78500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-78500.pth
Episode: 78500 Reward: 610.565 Loss: 73.814
episode: 79000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-79000.pth
Episode: 79000 Reward: 281.496 Loss: 26.555
episode: 79500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-79500.pth
Episode: 79500 Reward: 530.211 Loss: 22.184
episode: 80000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_Nones/model-80000.pth
Episode: 80000 Reward: 217.573 Loss: 120.436
