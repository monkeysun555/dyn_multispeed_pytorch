player initial finish
episode: 500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-500.pth
Episode: 500 Reward: -415.939 Loss: 14.472
episode: 1000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-1000.pth
Episode: 1000 Reward: -1163.953 Loss: 24.754
episode: 1500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-1500.pth
Episode: 1500 Reward: -369.056 Loss: 36.287
episode: 2000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-2000.pth
Episode: 2000 Reward: -931.620 Loss: 37.113
episode: 2500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-2500.pth
Episode: 2500 Reward: -192.104 Loss: 42.413
episode: 3000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-3000.pth
Episode: 3000 Reward: -105.104 Loss: 20.203
episode: 3500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-3500.pth
Episode: 3500 Reward: -56.810 Loss: 21.560
episode: 4000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-4000.pth
Episode: 4000 Reward: -144.526 Loss: 33.860
episode: 4500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-4500.pth
Episode: 4500 Reward: 26.907 Loss: 37.379
episode: 5000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-5000.pth
Episode: 5000 Reward: -115.609 Loss: 24.637
episode: 5500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-5500.pth
Episode: 5500 Reward: 87.996 Loss: 36.820
episode: 6000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-6000.pth
Episode: 6000 Reward: 111.281 Loss: 30.972
episode: 6500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-6500.pth
Episode: 6500 Reward: 245.882 Loss: 31.263
episode: 7000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-7000.pth
Episode: 7000 Reward: 65.616 Loss: 35.511
episode: 7500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-7500.pth
Episode: 7500 Reward: 151.229 Loss: 28.922
episode: 8000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-8000.pth
Episode: 8000 Reward: 176.577 Loss: 20.452
episode: 8500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-8500.pth
Episode: 8500 Reward: 263.550 Loss: 34.928
episode: 9000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-9000.pth
Episode: 9000 Reward: 172.866 Loss: 27.981
episode: 9500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-9500.pth
Episode: 9500 Reward: 6.572 Loss: 43.911
episode: 10000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-10000.pth
Episode: 10000 Reward: 280.852 Loss: 20.413
episode: 10500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-10500.pth
Episode: 10500 Reward: 168.146 Loss: 21.066
episode: 11000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-11000.pth
Episode: 11000 Reward: 60.054 Loss: 24.460
episode: 11500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-11500.pth
Episode: 11500 Reward: 409.052 Loss: 19.556
episode: 12000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-12000.pth
Episode: 12000 Reward: 217.769 Loss: 19.508
episode: 12500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-12500.pth
Episode: 12500 Reward: 411.537 Loss: 27.723
episode: 13000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-13000.pth
Episode: 13000 Reward: -43.215 Loss: 33.128
episode: 13500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-13500.pth
Episode: 13500 Reward: 322.899 Loss: 20.045
episode: 14000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-14000.pth
Episode: 14000 Reward: 404.121 Loss: 23.442
episode: 14500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-14500.pth
Episode: 14500 Reward: 298.171 Loss: 34.093
episode: 15000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-15000.pth
Episode: 15000 Reward: 363.330 Loss: 29.004
episode: 15500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-15500.pth
Episode: 15500 Reward: 404.080 Loss: 17.770
episode: 16000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-16000.pth
Episode: 16000 Reward: 460.127 Loss: 20.001
episode: 16500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-16500.pth
Episode: 16500 Reward: 317.405 Loss: 18.055
episode: 17000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-17000.pth
Episode: 17000 Reward: 445.909 Loss: 26.269
episode: 17500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-17500.pth
Episode: 17500 Reward: 424.527 Loss: 28.017
episode: 18000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-18000.pth
Episode: 18000 Reward: 234.245 Loss: 23.930
episode: 18500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-18500.pth
Episode: 18500 Reward: 256.923 Loss: 26.854
episode: 19000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-19000.pth
Episode: 19000 Reward: 260.903 Loss: 26.669
episode: 19500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-19500.pth
Episode: 19500 Reward: 481.664 Loss: 23.625
episode: 20000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-20000.pth
Episode: 20000 Reward: 319.578 Loss: 20.223
episode: 20500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-20500.pth
Episode: 20500 Reward: 302.004 Loss: 22.222
episode: 21000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-21000.pth
Episode: 21000 Reward: 237.623 Loss: 26.064
episode: 21500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-21500.pth
Episode: 21500 Reward: 273.656 Loss: 16.166
episode: 22000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-22000.pth
Episode: 22000 Reward: 444.277 Loss: 18.255
episode: 22500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-22500.pth
Episode: 22500 Reward: 475.145 Loss: 20.012
episode: 23000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-23000.pth
Episode: 23000 Reward: 340.566 Loss: 17.623
episode: 23500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-23500.pth
Episode: 23500 Reward: 281.899 Loss: 22.944
episode: 24000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-24000.pth
Episode: 24000 Reward: 483.219 Loss: 22.477
episode: 24500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-24500.pth
Episode: 24500 Reward: 294.636 Loss: 46.162
episode: 25000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-25000.pth
Episode: 25000 Reward: 339.166 Loss: 35.404
episode: 25500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-25500.pth
Episode: 25500 Reward: 162.054 Loss: 25.062
episode: 26000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-26000.pth
Episode: 26000 Reward: 359.130 Loss: 16.811
episode: 26500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-26500.pth
Episode: 26500 Reward: 501.168 Loss: 22.834
episode: 27000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-27000.pth
Episode: 27000 Reward: 59.442 Loss: 25.492
episode: 27500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-27500.pth
Episode: 27500 Reward: 128.526 Loss: 25.337
episode: 28000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-28000.pth
Episode: 28000 Reward: 331.512 Loss: 35.468
episode: 28500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-28500.pth
Episode: 28500 Reward: 309.927 Loss: 22.148
episode: 29000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-29000.pth
Episode: 29000 Reward: 313.818 Loss: 10.678
episode: 29500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-29500.pth
Episode: 29500 Reward: 490.242 Loss: 23.702
episode: 30000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-30000.pth
Episode: 30000 Reward: 246.455 Loss: 18.258
episode: 30500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-30500.pth
Episode: 30500 Reward: 88.362 Loss: 21.065
episode: 31000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-31000.pth
Episode: 31000 Reward: 114.858 Loss: 20.962
episode: 31500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-31500.pth
Episode: 31500 Reward: 490.959 Loss: 22.629
episode: 32000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-32000.pth
Episode: 32000 Reward: 491.698 Loss: 32.414
episode: 32500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-32500.pth
Episode: 32500 Reward: 282.343 Loss: 31.888
episode: 33000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-33000.pth
Episode: 33000 Reward: 369.873 Loss: 28.898
episode: 33500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-33500.pth
Episode: 33500 Reward: 100.197 Loss: 23.568
episode: 34000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-34000.pth
Episode: 34000 Reward: 499.933 Loss: 24.757
episode: 34500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-34500.pth
Episode: 34500 Reward: 132.287 Loss: 18.754
episode: 35000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-35000.pth
Episode: 35000 Reward: 343.905 Loss: 30.458
episode: 35500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-35500.pth
Episode: 35500 Reward: 352.093 Loss: 41.254
episode: 36000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-36000.pth
Episode: 36000 Reward: 170.133 Loss: 29.947
episode: 36500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-36500.pth
Episode: 36500 Reward: 259.050 Loss: 26.300
episode: 37000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-37000.pth
Episode: 37000 Reward: 339.669 Loss: 31.682
episode: 37500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-37500.pth
Episode: 37500 Reward: 306.025 Loss: 25.338
episode: 38000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-38000.pth
Episode: 38000 Reward: 492.238 Loss: 17.807
episode: 38500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-38500.pth
Episode: 38500 Reward: 502.358 Loss: 16.176
episode: 39000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-39000.pth
Episode: 39000 Reward: 259.908 Loss: 23.769
episode: 39500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-39500.pth
Episode: 39500 Reward: 48.659 Loss: 17.542
episode: 40000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-40000.pth
Episode: 40000 Reward: 494.575 Loss: 20.149
episode: 40500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-40500.pth
Episode: 40500 Reward: 499.393 Loss: 20.620
episode: 41000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-41000.pth
Episode: 41000 Reward: 42.269 Loss: 17.795
episode: 41500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-41500.pth
Episode: 41500 Reward: 496.638 Loss: 26.638
episode: 42000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-42000.pth
Episode: 42000 Reward: 334.201 Loss: 41.296
episode: 42500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-42500.pth
Episode: 42500 Reward: 490.207 Loss: 22.879
episode: 43000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-43000.pth
Episode: 43000 Reward: 171.642 Loss: 22.224
episode: 43500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-43500.pth
Episode: 43500 Reward: 209.365 Loss: 20.601
episode: 44000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-44000.pth
Episode: 44000 Reward: 297.496 Loss: 22.038
episode: 44500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-44500.pth
Episode: 44500 Reward: 473.521 Loss: 25.244
episode: 45000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-45000.pth
Episode: 45000 Reward: 328.424 Loss: 19.901
episode: 45500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-45500.pth
Episode: 45500 Reward: 482.901 Loss: 20.845
episode: 46000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-46000.pth
Episode: 46000 Reward: 98.585 Loss: 21.289
episode: 46500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-46500.pth
Episode: 46500 Reward: 465.430 Loss: 23.847
episode: 47000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-47000.pth
Episode: 47000 Reward: 195.989 Loss: 18.748
episode: 47500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-47500.pth
Episode: 47500 Reward: 192.155 Loss: 19.752
episode: 48000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-48000.pth
Episode: 48000 Reward: 458.190 Loss: 26.971
episode: 48500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-48500.pth
Episode: 48500 Reward: 330.101 Loss: 19.400
episode: 49000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-49000.pth
Episode: 49000 Reward: 381.646 Loss: 19.027
episode: 49500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-49500.pth
Episode: 49500 Reward: 201.888 Loss: 27.706
episode: 50000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-50000.pth
Episode: 50000 Reward: 501.200 Loss: 17.170
episode: 50500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-50500.pth
Episode: 50500 Reward: 276.502 Loss: 40.214
episode: 51000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-51000.pth
Episode: 51000 Reward: 125.610 Loss: 24.774
episode: 51500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-51500.pth
Episode: 51500 Reward: 468.488 Loss: 23.668
episode: 52000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-52000.pth
Episode: 52000 Reward: 483.114 Loss: 21.400
episode: 52500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-52500.pth
Episode: 52500 Reward: 129.530 Loss: 16.702
episode: 53000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-53000.pth
Episode: 53000 Reward: 271.749 Loss: 32.348
episode: 53500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-53500.pth
Episode: 53500 Reward: 477.938 Loss: 30.470
episode: 54000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-54000.pth
Episode: 54000 Reward: 139.775 Loss: 18.252
episode: 54500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-54500.pth
Episode: 54500 Reward: 160.925 Loss: 34.554
episode: 55000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-55000.pth
Episode: 55000 Reward: 108.143 Loss: 24.393
episode: 55500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-55500.pth
Episode: 55500 Reward: 359.240 Loss: 28.382
episode: 56000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-56000.pth
Episode: 56000 Reward: 486.970 Loss: 22.920
episode: 56500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-56500.pth
Episode: 56500 Reward: 458.053 Loss: 31.545
episode: 57000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-57000.pth
Episode: 57000 Reward: 470.273 Loss: 18.403
episode: 57500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-57500.pth
Episode: 57500 Reward: 206.308 Loss: 26.378
episode: 58000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-58000.pth
Episode: 58000 Reward: 500.093 Loss: 28.833
episode: 58500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-58500.pth
Episode: 58500 Reward: 197.676 Loss: 21.654
episode: 59000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-59000.pth
Episode: 59000 Reward: 489.710 Loss: 23.185
episode: 59500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-59500.pth
Episode: 59500 Reward: 318.246 Loss: 19.565
episode: 60000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-60000.pth
Episode: 60000 Reward: 117.734 Loss: 19.937
episode: 60500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-60500.pth
Episode: 60500 Reward: 327.919 Loss: 24.644
episode: 61000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-61000.pth
Episode: 61000 Reward: 488.693 Loss: 25.829
episode: 61500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-61500.pth
Episode: 61500 Reward: 347.969 Loss: 30.255
episode: 62000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-62000.pth
Episode: 62000 Reward: 317.895 Loss: 19.532
episode: 62500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-62500.pth
Episode: 62500 Reward: 34.658 Loss: 25.872
episode: 63000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-63000.pth
Episode: 63000 Reward: 495.254 Loss: 20.877
episode: 63500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-63500.pth
Episode: 63500 Reward: 473.090 Loss: 20.566
episode: 64000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-64000.pth
Episode: 64000 Reward: 347.783 Loss: 28.196
episode: 64500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-64500.pth
Episode: 64500 Reward: 274.491 Loss: 17.920
episode: 65000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-65000.pth
Episode: 65000 Reward: 477.942 Loss: 22.826
episode: 65500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-65500.pth
Episode: 65500 Reward: 500.192 Loss: 14.417
episode: 66000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-66000.pth
Episode: 66000 Reward: 328.484 Loss: 24.174
episode: 66500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-66500.pth
Episode: 66500 Reward: 297.694 Loss: 21.799
episode: 67000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-67000.pth
Episode: 67000 Reward: 134.075 Loss: 16.955
episode: 67500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-67500.pth
Episode: 67500 Reward: 302.682 Loss: 15.011
episode: 68000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-68000.pth
Episode: 68000 Reward: 314.924 Loss: 28.466
episode: 68500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-68500.pth
Episode: 68500 Reward: 480.131 Loss: 26.334
episode: 69000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-69000.pth
Episode: 69000 Reward: 477.070 Loss: 29.693
episode: 69500
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-69500.pth
Episode: 69500 Reward: 503.330 Loss: 23.944
episode: 70000
=> Save ./models/logs_m_2/q_2/t_2/l_0/latency_3s/model-70000.pth
Episode: 70000 Reward: 281.574 Loss: 27.574
